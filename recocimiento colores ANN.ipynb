{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959ad801",
   "metadata": {},
   "source": [
    "### importamos las librerias a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab0dac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Dropout\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012439b",
   "metadata": {},
   "source": [
    "### Preparacion de espacio de trabajo\n",
    "\n",
    "especificamos la ruta de nuestro dataset, un arreglo para guardar las categorias que vayamos encontrando y la dimension a trabajar con nuestras imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6685ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/home/lenin/Documents/datasets/colores'\n",
    "CATEGORIES = []\n",
    "IMG_SIZE=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e7850",
   "metadata": {},
   "source": [
    "recorremos el direcotorio excluyendo las imagenes de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fba52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorias encontradas: ['morado', 'azul', 'rojo', 'amarillo', 'gris', 'rosado', 'negro', 'naranja', 'celeste', 'verde', 'blanco', 'cafe']\n",
      " total: 12\n"
     ]
    }
   ],
   "source": [
    "for cate in os.listdir(DATADIR):\n",
    "    if cate == 'testimg': continue\n",
    "    CATEGORIES.append(cate)\n",
    "print(f'categorias encontradas: {CATEGORIES}\\n total: {len(CATEGORIES)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639757ce",
   "metadata": {},
   "source": [
    "### Recoleccion de datos\n",
    "recorremos el directorio tomando cada imagen segun su respectiva categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614c1cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenin/.local/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "training_data=[] #var para los datos recolectados\n",
    "labels=0\n",
    "for category in CATEGORIES:\n",
    "    path=os.path.join(DATADIR, category)\n",
    "    for fname in os.listdir(path):\n",
    "        img = load_img((path+'/'+fname), target_size=(IMG_SIZE,IMG_SIZE))\n",
    "        x = img_to_array(img)\n",
    "        x=x/255\n",
    "        training_data.append([x,labels])\n",
    "    labels+=1\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df437530",
   "metadata": {},
   "source": [
    "verificamos el total de datos que tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376f016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50400\n"
     ]
    }
   ],
   "source": [
    "lenofimage = len(training_data)\n",
    "print(lenofimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafca375",
   "metadata": {},
   "source": [
    "### Tratamiento de los datos\n",
    "separamos  nuestra data en img-labels y los tratamos con numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf3f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for img, label in training_data:\n",
    "    X.append(img)\n",
    "    y.append(label)\n",
    "print('done')\n",
    "del training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a4673",
   "metadata": {},
   "source": [
    "verificamos la forma de nuestra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4777655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels (50400,)\n",
      "img (50400, 5, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "print('labels',y.shape)\n",
    "print('img',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c237d2a",
   "metadata": {},
   "source": [
    "dividimos la data para train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f07e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 37800, test: 12600\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "print(f'train: {len(X_train)}, test: {len(X_test)}')\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de46ea7",
   "metadata": {},
   "source": [
    "### Creacion del modelo\n",
    "creamos la arquitectura del modelo, lo compilamos y finalmente ajustamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fcbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(X_train[0].shape)))\n",
    "# model.add(Dense(len(CATEGORIES), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13233a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (2,2), activation='relu', padding='SAME', input_shape=X_train[0].shape),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(len(CATEGORIES), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab8aff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='SAME', input_shape=X_train[0].shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='SAME', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb69ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "756/756 [==============================] - 5s 4ms/step - loss: 0.7498 - accuracy: 0.7576\n",
      "Epoch 2/50\n",
      "756/756 [==============================] - 4s 5ms/step - loss: 0.3685 - accuracy: 0.8666\n",
      "Epoch 3/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.3417 - accuracy: 0.8718\n",
      "Epoch 4/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.3284 - accuracy: 0.8754\n",
      "Epoch 5/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.3209 - accuracy: 0.8767\n",
      "Epoch 6/50\n",
      "756/756 [==============================] - 4s 5ms/step - loss: 0.3152 - accuracy: 0.8794\n",
      "Epoch 7/50\n",
      "756/756 [==============================] - 4s 5ms/step - loss: 0.3077 - accuracy: 0.8820\n",
      "Epoch 8/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.3001 - accuracy: 0.8854\n",
      "Epoch 9/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2961 - accuracy: 0.8852\n",
      "Epoch 10/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2899 - accuracy: 0.8884\n",
      "Epoch 11/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2855 - accuracy: 0.8898\n",
      "Epoch 12/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2792 - accuracy: 0.8929\n",
      "Epoch 13/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2778 - accuracy: 0.8925\n",
      "Epoch 14/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2720 - accuracy: 0.8940\n",
      "Epoch 15/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2719 - accuracy: 0.8953\n",
      "Epoch 16/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2684 - accuracy: 0.8963\n",
      "Epoch 17/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2660 - accuracy: 0.8970\n",
      "Epoch 18/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2639 - accuracy: 0.8999\n",
      "Epoch 19/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2603 - accuracy: 0.9004\n",
      "Epoch 20/50\n",
      "756/756 [==============================] - 2s 3ms/step - loss: 0.2590 - accuracy: 0.9017\n",
      "Epoch 21/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2556 - accuracy: 0.9022\n",
      "Epoch 22/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2550 - accuracy: 0.9016\n",
      "Epoch 23/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2498 - accuracy: 0.9044\n",
      "Epoch 24/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2499 - accuracy: 0.9038\n",
      "Epoch 25/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2459 - accuracy: 0.9052\n",
      "Epoch 26/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2459 - accuracy: 0.9062\n",
      "Epoch 27/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2441 - accuracy: 0.9066\n",
      "Epoch 28/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2403 - accuracy: 0.9073\n",
      "Epoch 29/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2418 - accuracy: 0.9083\n",
      "Epoch 30/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2376 - accuracy: 0.9099\n",
      "Epoch 31/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2342 - accuracy: 0.9100\n",
      "Epoch 32/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2353 - accuracy: 0.9090\n",
      "Epoch 33/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2360 - accuracy: 0.9101\n",
      "Epoch 34/50\n",
      "756/756 [==============================] - 4s 6ms/step - loss: 0.2336 - accuracy: 0.9111\n",
      "Epoch 35/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2291 - accuracy: 0.9138\n",
      "Epoch 36/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2262 - accuracy: 0.9136\n",
      "Epoch 37/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2268 - accuracy: 0.9131\n",
      "Epoch 38/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2240 - accuracy: 0.9144\n",
      "Epoch 39/50\n",
      "756/756 [==============================] - 3s 4ms/step - loss: 0.2220 - accuracy: 0.9145\n",
      "Epoch 40/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2231 - accuracy: 0.9153\n",
      "Epoch 41/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2206 - accuracy: 0.9153\n",
      "Epoch 42/50\n",
      "756/756 [==============================] - 2s 3ms/step - loss: 0.2194 - accuracy: 0.9166\n",
      "Epoch 43/50\n",
      "756/756 [==============================] - 2s 3ms/step - loss: 0.2175 - accuracy: 0.9169\n",
      "Epoch 44/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2168 - accuracy: 0.9170\n",
      "Epoch 45/50\n",
      "756/756 [==============================] - 2s 3ms/step - loss: 0.2138 - accuracy: 0.9177\n",
      "Epoch 46/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2146 - accuracy: 0.9196\n",
      "Epoch 47/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2102 - accuracy: 0.9193\n",
      "Epoch 48/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2086 - accuracy: 0.9197\n",
      "Epoch 49/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2084 - accuracy: 0.9204\n",
      "Epoch 50/50\n",
      "756/756 [==============================] - 3s 3ms/step - loss: 0.2051 - accuracy: 0.9214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb43c627160>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=55, batch_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99addcd6",
   "metadata": {},
   "source": [
    "verificamos la precision en testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05634151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.20547828078269958\n",
      "test accuracy 0.925000011920929\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('test loss',test_loss)\n",
    "print('test accuracy',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906391c4",
   "metadata": {},
   "source": [
    "### Probamos el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1430fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediccion = rosado\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ruta a nuestras imagenes de test en el directorio, no de la data preparada\n",
    "path = DATADIR + '/testimg' + '/10.jpeg' \n",
    "\n",
    "#tratamos la img con el mismo tamanio y la normalizamos\n",
    "img = load_img(path, target_size=(IMG_SIZE,IMG_SIZE))\n",
    "x = img_to_array(img)\n",
    "x=x/255\n",
    "\n",
    "#agrego un eje para que el modelo lo reciba\n",
    "x = x[np.newaxis, ...]\n",
    "\n",
    "#imprimo la img y la prediccion\n",
    "plt.imshow(img)\n",
    "resp = model.predict(x, verbose=0)\n",
    "print(f'prediccion = {CATEGORIES[np.argmax(resp[0])]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b62849",
   "metadata": {},
   "source": [
    "### Exportacion\n",
    "con el modelo ya funcionando correctamente, lo exportamos para poder utilizarlo en otros proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d6e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('mod_color_v1.h5')\n",
    "# model.save_weights(\"mod_color_v1_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281d610",
   "metadata": {},
   "source": [
    "para cargarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d3444ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cargar la estructura del modelo\n",
    "# modelo_cargado = tf.keras.models.load_model(\"mod_color_v1.h5\")\n",
    "\n",
    "# # Cargar los pesos del modelo\n",
    "# modelo_cargado.load_weights(\"mod_color_v1_weights.h5\")\n",
    "convd \n",
    "drop out \n",
    "max poling \n",
    "padding \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
